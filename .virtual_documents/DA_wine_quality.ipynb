##DATA MANIPULATION
import pandas as pd
import numpy as np
#DATA VISUALIZATION
import seaborn as sns
import matplotlib.pyplot as plt
#DATA PROCESSING
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, classification_report
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier







wine_dataset = pd.read_csv('winequality-dataset_updated.csv')


wine_dataset.dtypes


wine_dataset.isna().any(axis=1).sum()


sns.histplot(data=wine_dataset,x='quality',bins=10,kde=True)


wine_dataset.describe()





def limit_outliers(df):
    df_clean = df.copy()
    for col in ['residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide']:
        Q1 = df_clean[col].quantile(.25)
        Q3 = df_clean[col].quantile(.75)
        IQR = Q3-Q1
        upper_limit = Q3 +1.5*IQR
        lower_limit = Q1 - 1.5*IQR
        
        df_clean.loc[df_clean[col]>upper_limit] = upper_limit
        df_clean.loc[df_clean[col]<lower_limit]= lower_limit
    return df_clean
        


df_clean = limit_outliers(wine_dataset)


df_clean['quality'] = np.where(df_clean['quality']>=7, 1, 0)


df_clean['quality'].value_counts(normalize=True)





X= df_clean.drop(columns=['quality'])
y = df_clean['quality']


X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.20,random_state=42,stratify=y)


X_tr,X_val,y_tr,y_val = train_test_split(X_train,y_train,test_size=.25,random_state=42,stratify=y_train)


%%time
cv_para={
    'n_estimators' : [100,200,300],
    'max_depth':[None,3,5,7],
    'min_samples_split': [5,7,10],
    'min_samples_leaf':[1,5,4],
    'max_features': ['sqrt',None,'log2']
    
}
scores = ['f1','recall','precision','accuracy']

rf_clf = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(estimator=rf_clf,
                          scoring=scores,
                          refit='f1',
                          cv=5,
                          param_grid=cv_para,
                          n_jobs=1)

grid_search.fit(X_train,y_train)


grid_search.best_score_


import pickle
with open('rfc.pickle', mode='wb') as file:
    pickle.dump(grid_search,file)


y_pred = grid_search.predict(X_test)


cm = confusion_matrix(y_test,y_pred)


cmd = ConfusionMatrixDisplay(cm)
cmd.plot()


cm


print(classification_report(y_test,y_pred))


from sklearn.inspection import plot_partial_dependence

# Graficar efectos parciales
plot_partial_dependence(model, X_train, features=feature_importance_df['Feature'][:5])  # Muestra las 5 características más importantes
plt.show()



